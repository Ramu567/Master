 Create Managed_Table:
 ---------------------
1) CREATE TABLE student (name STRING,id int, marks int) row format delimited fields  terminated by ',';
LOAD DATA local INPATH '/tmp/Student.csv' OVERWRITE INTO table student;

2) CREATE TABLE student_two (stname STRING,st_id int, marks int) row format delimited fields terminated by ',';
LOAD DATA LOCAL INPATH "/tmp/student_new.csv" INTO TABLE student_two;
 ==>These tables will be  stored in warehouse floder in HDFC .
 
Droping a Manage Table:
----------------------
DROP TABLE managed_table name;
Ex: drop table student;
 ==>If you drop managed table it will perminetly deleted from warehouse floder in HDFS And also delete matadata from derby database .
---------------------------------------------------------------

Creating External Tables:
--------------------------
create EXTERNAL table txnrecords(txnno INT, txndate varchar(100), custno INT, amount DOUBLE,category varchar(100),product varchar(100),city varchar(100), state varchar(100), spendby varchar(100)) row format delimited fields terminated by ','LOCATION '/hive/txnrecords/';
LOAD DATA  inpath '/sqoop/txnrecords/part-m-00000' INTO TABLE txnrecords;

create EXTERNAL table emp(id int,name varchar(30),job varchar(25),salary float,deptno varchar(3))row format delimited fields terminated by ','LOCATION '/hive/emp/';
LOAD DATA  inpath '/sqoop/emp/' INTO TABLE emp;

create table paymentinfo(RefNo int,EXAMINER varchar(30),BANKNUMBER varchar(25),DOCUMENTTYPE varchar(15),PAYMENTACTION varchar(15),CHECKNUMBER int,PAYEEID int,PAYCODE int,DATEPRINT DATE,DATEISSUED DATE,DATEPAYFROM DATE,DATEPAYTHRU DATE,NETAMOUNT float)row format delimited fields terminated by ','LOCATION '/hive/paymentinfo/';
LOAD DATA  inpath '/sqoop/paymentinfo/part-m-00000' INTO TABLE paymentinfo;

1)CREATE EXTERNAL TABLE oper_new (empname string,empid int,year int) row format delimited fields terminated by ','LOCATION '/example/techworm/';
LOAD DATA local INPATH '/tmp/student_new.csv' INTO TABLE oper_new;
create external table wc2015runs(player string,mat int,inns int,no int,runs int,HS int,avg float,BF float,SR int,hundreds int,fiftys int,zeros int,fours int,sixs int) row format delimited fields terminated by '\t' STORED AS textfile LOCATION '/hive/wc2015runs/';
load data inpath '/ramu/wc2015rus.txt' into table wc2015runs;
 
==>These tables will be stored in hive database and external Location(means where ever you put tables.) 

Drop External Table:
--------------------
DROP TABLE oper_new;

If you drop the table in external process.that table  will delete from hive databes but not from external location.
-------------------------------------------------------------------
2)CREATE EXTERNAL TABLE student (stname STRING,stno int,totmarks int)LOCATION '/user/hive/tables' row format delimited fields terminated by ',';
LOAD DATA INPATH '/user/hive/data/student.csv' INTO TABLE student;
-------------------------------------------------------------------
==> By configuring  this property in hive-site.xml (or)  set the "set hive.cli.print.header=true;"  in console we can see the schama of a table along with data.
<name>hive.cli.print.header</name>
true
(or)
set hive.cli.print.header=true;
-------------------------------------------------------------------
OVERWRITE INTO TABLE ; 
1)LOAD DATA LOCAL INPATH '/tmp/samplecode.csv' OVERWRITE INTO TABLE demotable;
2)LOAD DATA INPATH '/user/hive/data/samplecode.csv' OVERWRITE INTO TABLE demotable;
-------------------------------------------------------------------
Altering Table :
================
1)	ALTER TABLE student_update  ADD COLUMNS (std_add string);
Like that do all the alter operation what ever supports in mysql or Oracle etc.
-------------------------------------------------------------------
 
 
 Custom (user-defind)PARTITIONS IN HIVE
 ---------------------------------------
1)CREATE TABLE student_part (name STRING,id int) PARTITIONED BY (edate STRING) row format delimited fields terminated by ',';
 LOAD DATA LOCAL INPATH '/tmp/student_2011.csv' OVERWRITE INTO TABLE student_part PARTITION (edate='2011');
 
2)create table tmp(name STRING,id int,creationdate string) row format delimited fields terminated by ',';
LOAD DATA LOCAL INPATH '/tmp/student_new.csv' OVERWRITE INTO TABLE tmp;

3)CREATE TABLE student_part (name STRING,id int) PARTITIONED BY (ds STRING);
INSERT OVERWRITE into TABLE student_part PARTITION(ds='2013' ) SELECT name,id,creationdate FROM tmp  WHERE creationdate='2013';
soap prod:
---------
4)create  external table soap(pid int,pname string,cost int,quantity int,color string,pcode int)PARTITIONED BY (country string,state string) row format delimited fields terminated by ',' location '/hive/soap/';
LOAD DATA INPATH '/ravi/prod/prod1.txt' OVERWRITE INTO TABLE soap PARTITION (country='India',state='AndhraPradesh');
LOAD DATA INPATH '/ravi/prod/prod2.txt' OVERWRITE INTO TABLE soap PARTITION (country='India',state='Telengana');
LOAD DATA INPATH '/ravi/prod/prod3.txt' OVERWRITE INTO TABLE soap PARTITION (country='India',state='Karanataka');
 when you are  trying get all records of particluar  partition,hive will not run any mapreduce programs it will just like select*from tablename.
 
select*from soap where state ='AndhraPradesh';
select*from soap where state ='Karanataka';
select*from soap where state ='Telengana';

 
To show the all the partitions of the table:
---------------------------------------------
show partitions soap;
show partitions txnrecordsbycat;

Creating Dynamic Partitions:
------------------------------
To enable dynamic partitions
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=300;
set hive.enforce.bucketing = true;
set hive.exec.max.created.files=150000;

    We are using the dynamic partition without a static partition (A table can be partitioned based on multiple columns in hive) in such case we have to enable the non- strict mode. In strict mode we can use dynamic partition only with out a Static Partition.
set hive.exec.max.dynamic.partitions.pernode=300;

  The default value is 100, we have to modify the same according to the possible no of partitions that would come in your case
set hive.exec.max.created.files=150000;

creating dynamic tables:
 ---------------------------
 
create EXTERNAL table txnrecords(txnno INT, txndate varchar(100), custno INT, amount DOUBLE,category varchar(100),product varchar(100),city varchar(100), state varchar(100), spendby varchar(100)) row format delimited fields terminated by ','LOCATION '/hive/txnrecords/';
LOAD DATA  inpath '/sqoop/txnrecords/part-m-00000' INTO TABLE txnrecords;

create table txnrecordsbycat(txnno INT, txndate STRING, custno INT, amount DOUBLE,product STRING, city STRING, state STRING, spendby STRING)partitioned by (category STRING)clustered by (state) INTO 10 buckets row format delimited fields terminated by ','stored as textfile;

load data to txnrecordsbycat:
-----------------------------
from txnrecords txn INSERT OVERWRITE TABLE txnrecordsbycat PARTITION(category)select txn.txnno, txn.txndate,txn.custno, txn.amount,txn.product,txn.city,txn.state,txn.spendby, txn.category DISTRIBUTE BY category;
To create a dynamic table first we have to create one temporary table  and load the data into temporary table  after by using that temporary table we can insert the data  from temporary table into dynamic table, becuase hive 
does not suppport directly load the data into to dynamic tables.
 
1)CREATE TABLE Stage_oper_Month(oper_id string,Creation_Date string,oper_name String,oper_age int,oper_dept String, oper_dept_id int,opr_status string,EYEAR STRING,EMONTH STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
LOAD DATA local INPATH '/tmp/user_info.csv'INTO TABLE  Stage_oper_Month; 

2)CREATE TABLE Fact_oper_Month(oper_id string,Creation_Date string,oper_name String,oper_age int,oper_dept String, oper_dept_id int,opr_status string)PARTITIONED BY(eyear STRING, eMONTH STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
FROM Stage_oper_Month 
INSERT OVERWRITE TABLE Fact_oper_Month
PARTITION(eyear,eMONTH)
SELECT  
oper_id,Creation_Date,oper_name,oper_age,oper_dept,oper_dept_id,opr_status,EYEAR,EMONTH
DISTRIBUTE BY eyear,eMONTH;

select oper_id,oper_name,oper_dept from Stage_oper_Month where eyear=2010 and emonth=1;
-------------------------------------Buckets ------------------------
Creating Buckets:
-----------------
CREATE TABLE Month_new (oper_id string,Creation_Date string,oper_name String,oper_age int,oper_dept String, oper_dept_id int,opr_status string,eyear string ,emonth string) CLUSTERED BY(oper_id) SORTED BY (oper_id,Creation_Date) INTO 4 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 
insert overwrite table Month_new
select symbol, exchange, price, volume, cond, bid, ask, time from trades
distribute by symbol sort by symbol, time;

INSERT OVERWRITE TABLE Month_new
SELECT 
oper_id,Creation_Date,oper_name,oper_age,oper_dept,oper_dept_id,opr_status,EYEAR,EMONTH FROM Stage_oper_Month 
DISTRIBUTE BY oper_id sort by oper_id,Creation_Date;

1)CREATE TABLE Operator_split1(oper_id int,oper_code string,agency_code int,oper_name String,agency_name String,Creation_Date string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; ---19912
LOAD DATA LOCAL INPATH '/tmp/oper_split.csv' OVERWRITE INTO TABLE Operator_split1 ;

2)CREATE TABLE operator_split_two(oper_id int,oper_code string,agency_code int,oper_name String,agency_name String,Creation_Date string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
LOAD DATA LOCAL INPATH '/tmp/oper_split_2.csv' OVERWRITE INTO TABLE operator_split_two ;

3)CREATE TABLE oper_bucket (oper_id int,oper_code string,agency_code int,oper_name String,agency_name String,Creation_Date string) CLUSTERED BY(oper_id) SORTED BY (oper_id,Creation_Date) INTO 6 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; 

INSERT OVERWRITE TABLE oper_bucket
SELECT 
oper_id,oper_code,agency_code,oper_name,agency_name,Creation_Date FROM operator_split_two
DISTRIBUTE BY oper_id sort by oper_id,Creation_Date;
select  agency_code ,encription(agency_name),count(oper_id) from oper_bucket group by agency_code,agency_name ;
select agency_code,encription(agency_name) from oper_bucket limit 10;
SELECT count(*) FROM oper_bucket TABLESAMPLE(BUCKET 1 OUT OF 6 ON oper_id) WHERE eyear=’2011’ AND eMONTH=’11’;
SELECT * FROM Month TABLESAMPLE(BUCKET 2 OUT OF 6 ON  rand()) WHERE eyear='2011';
SELECT * FROM Month TABLESAMPLE(2 PERCENT) ;
SELECT * FROM Month TABLESAMPLE ( 20 ROWS) WHERE eyear='2011' ;

TABLESAMPLE(BUCKET 3 OUT OF 16 ON id)
from tmp INSERT OVERWRITE TABLE student PARTITION(dt) select name ,id from student where creationdate='2013';
SET mapred.child.java.opts="-server -Xmx512M
CREATE  TABLE buckt_test (oper_id int,oper_code string,agency_code int,oper_name String,agency_name String,Creation_Date string) PARTITIONED BY (eyear STRING) CLUSTERED BY(oper_id) SORTED BY (oper_id) INTO 6 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

CREATE TABLE buckt_dump(oper_id int,oper_code string,agency_code int,oper_name String,agency_name String,Creation_Date string,eyear STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
LOAD DATA LOCAL INPATH '/tmp/oper_split.csv' OVERWRITE INTO TABLE buckt_dump ;

FROM buckt_dump 
INSERT OVERWRITE TABLE buckt_test
PARTITION(eyear=2013)
SELECT 
oper_id,oper_code,agency_code,oper_name,agency_name,Creation_Date,eyear 
DISTRIBUTE BY eyear,oper_id sort by oper_id;

INSERT OVERWRITE TABLE buckt_test
SELECT 
oper_id,oper_code,agency_code,oper_name,agency_name,Creation_Date,eyear FROM buckt_dump 
DISTRIBUTE BY oper_id sort by oper_id,Creation_Date;
LOCATION '/home/test_dir';

CREATE TABLE Fact_oper_Month(oer_id string,Creation_Date string,oper_name String,oper_age int,oper_dept String,oper_dept_id int,opr_status string)PARTITIONED BY(eyear STRING, eMONTH STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

CREATE  TABLE IF NOT EXISTS test_table(Id INT,name String,country string,continent string) PARTITIONED BY (dt STRING,hour STRING) CLUSTERED BY(country,continent) SORTED BY(country,continent) INTO 4 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

Creating Tables:
----------------
create tables users(id int, name String) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
LOAD DATA LOCAL INPATH '/tmp/users.csv' OVERWRITE INTO TABLE users ;

Crating index to Tables:
------------------------	

CREATE INDEX new_index ON TABLE users(id) AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler';

Creating View to Table:
----------------------

CREATE VIEW view_name (id,name)  AS SELECT * from users;
-------------------------------------------------------------------
select commands

SHOW PARTITIONS month_part;
 INSERT OVERWRITE LOCAL DIRECTORY '/tmp/tuser' SELECT * FROM student_update;
 
Creating a jar file:
--------------------

 add jar encription.jar
 add jar /home/hdpdev/code/encryption.jar
create temporary function encryption as 'Sha1encryption';
select oper_id,encription(oper_name) from Fact_oper_Month limit 10;
select oper_id,oper_name from Fact_oper_Month limit 10;
create temporary function encription as 'Sha1encryption';

1)CREATE TABLE Stage_oper_Month_new(oper_id string,Creation_Date string,oper_name String,oper_age int,oper_dept String, oper_dept_id int,opr_status string,EYEAR STRING,EMONTH STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
select oper_id,oper_name,oper_age from Stage_oper_Month where EYEAR='2011'
select oper_id,oper_name,oper_age from Fact_oper_Month where EYEAR='2011' and  EMONTH='9';

==================================================================================
hive> show create table emp;

OK
CREATE EXTERNAL TABLE `emp`(
  `id` int, 
  `name` varchar(30), 
  `job` varchar(25), 
  `salary` float, 
  `deptno` varchar(3))
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
WITH SERDEPROPERTIES ( 
  'field.delim'=',', 
  'serialization.format'=',') 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://quickstart.cloudera:8020/hive/emp'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='false', 
  'numFiles'='0', 
  'numRows'='-1', 
  'rawDataSize'='-1', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1478341960')
Time taken: 1.14 seconds, Fetched: 24 row(s)

====================================================
hive> describe formatted emp;
OK
col_name	data_type	comment
# col_name            	data_type           	comment             
	 	 
id                  	int                 	                    
name                	varchar(30)         	                    
job                 	varchar(25)         	                    
salary              	float               	                    
deptno              	varchar(3)          	                    
	 	 
# Detailed Table Information	 	 
Database:           	ramu                	 
Owner:              	cloudera            	 
CreateTime:         	Sat Nov 05 03:32:40 PDT 2016	 
LastAccessTime:     	UNKNOWN             	 
Protect Mode:       	None                	 
Retention:          	0                   	 
Location:           	hdfs://quickstart.cloudera:8020/hive/emp	 
Table Type:         	EXTERNAL_TABLE      	 
Table Parameters:	 	 
	COLUMN_STATS_ACCURATE	false               
	EXTERNAL            	TRUE                
	numFiles            	0                   
	numRows             	-1                  
	rawDataSize         	-1                  
	totalSize           	0                   
	transient_lastDdlTime	1478341960          
	 	 
# Storage Information	 	 
SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	 
InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	 
OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	 
Compressed:         	No                  	 
Num Buckets:        	-1                  	 
Bucket Columns:     	[]                  	 
Sort Columns:       	[]                  	 
Storage Desc Params:	 	 
	field.delim         	,                   
	serialization.format	,                   
Time taken: 0.312 seconds, Fetched: 37 row(s)
================================================================
hive> explain select*from emp;

OK
Explain
STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: emp
          Statistics: Num rows: 2 Data size: 145 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: id (type: int), name (type: varchar(30)), job (type: varchar(25)), salary (type: float), deptno (type: varchar(3))
            outputColumnNames: _col0, _col1, _col2, _col3, _col4
            Statistics: Num rows: 2 Data size: 145 Basic stats: COMPLETE Column stats: NONE
            ListSink

Time taken: 1.692 seconds, Fetched: 17 row(s)
======================================================================
