hadoop fs -cat /user/cloudera/pyspark/departmentsJson/part*
  
  ##############################################################################
 +# Developing word count program
 +# Create a file and type few lines and save it as wordcount.txt and copy to HDFS
 +# to /user/cloudera/wordcount.txt
 +
 +data = sc.textFile("/user/cloudera/wordcount.txt")
 +dataFlatMap = data.flatMap(lambda x: x.split(" "))
 +dataMap = dataFlatMap.map(lambda x: (x, 1))
 +dataReduceByKey = dataMap.reduceByKey(lambda x,y: x + y)
 +
 +dataReduceByKey.saveAsTextFile("/user/cloudera/wordcountoutput")
 +
 +for i in dataReduceByKey.collect():
 +  print(i)
 +
 +##############################################################################
  
  # Join disparate datasets together using Spark
 -# Problem statement, get the revenue and number of orders on daily basis
 +# Problem statement, get the revenue and number of orders from order_items on daily basis
  ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
  orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
  
 -ordersParsedRDD = ordersRDD.map(lambda rec: (rec.split(",")[0], rec))
 -orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (rec.split(",")[1], rec))
 +ordersParsedRDD = ordersRDD.map(lambda rec: (int(rec.split(",")[0]), rec))
 +orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[1]), rec))
  
 -ordersJoinOrderItems = ordersParsedRDD.join(orderItemsParsedRDD)
 -joinData = ordersJoinOrderItems.map(lambda t: (t[1][0].split(",")[1], float(t[1][1].split(",")[4])))
 +ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
 +revenuePerOrderPerDay = ordersJoinOrderItems.map(lambda t: (t[1][1].split(",")[1], float(t[1][0].split(",")[4])))
  
 -ordersAggRDD = joinData.aggregateByKey( \
 -(0, 0), \
 -lambda subtotal1, subtotal2: (subtotal1[0] + subtotal2, subtotal1[1] + 1), \
 -lambda total1, total2: (round(total1[0] + total2[0], 2), total1[1] + total2[1]) \
 +# Get order count per day
 +ordersPerDay = ordersJoinOrderItems.map(lambda rec: rec[1][1].split(",")[1] + "," + str(rec[0])).distinct()
 +ordersPerDayParsedRDD = ordersPerDay.map(lambda rec: (rec.split(",")[0], 1))
 +totalOrdersPerDay = ordersPerDayParsedRDD.reduceByKey(lambda x, y: x + y)
 +
 +# Get revenue per day from joined data
 +totalRevenuePerDay = revenuePerOrderPerDay.reduceByKey( \
 +lambda total1, total2: total1 + total2 \
  )
  
 -for data in ordersAggRDD.collect():
 +for data in totalRevenuePerDay.collect():
 +  print(data)
 +
 +# Joining order count per day and revenue per day
 +finalJoinRDD = totalOrdersPerDay.join(totalRevenuePerDay)
 +for data in finalJoinRDD.take(5):
 +  print(data)
 +
 +# Using Hive
 +from pyspark.sql import HiveContext
 +sqlContext = HiveContext(sc)
 +sqlContext.sql("set spark.sql.shuffle.partitions=10");
 +
 +joinAggData = sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), \
 +count(distinct o.order_id) from orders o join order_items oi \
 +on o.order_id = oi.order_item_order_id \
 +group by o.order_date order by o.order_date")
 +
 +for data in joinAggData.collect():
 +  print(data)
 +
 +# Using spark native sql
 +from pyspark.sql import SQLContext, Row
 +sqlContext = SQLContext(sc)
 +sqlContext.sql("set spark.sql.shuffle.partitions=10");
 +
 +ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
 +ordersMap = ordersRDD.map(lambda o: o.split(","))
 +orders = ordersMap.map(lambda o: Row(order_id=int(o[0]), order_date=o[1], \
 +order_customer_id=int(o[2]), order_status=o[3]))
 +ordersSchema = sqlContext.inferSchema(orders)
 +ordersSchema.registerTempTable("orders")
 +
 +orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
 +orderItemsMap = orderItemsRDD.map(lambda oi: oi.split(","))
 +orderItems = orderItemsMap.map(lambda oi: Row(order_item_id=int(oi[0]), order_item_order_id=int(oi[1]), \
 +order_item_product_id=int(oi[2]), order_item_quantity=int(oi[3]), order_item_subtotal=float(oi[4]), \
 +order_item_product_price=float(oi[5])))
 +orderItemsSchema = sqlContext.inferSchema(orderItems)
 +orderItemsSchema.registerTempTable("order_items")
 +
 +joinAggData = sqlContext.sql("select o.order_date, sum(oi.order_item_subtotal), \
 +count(distinct o.order_id) from orders o join order_items oi \
 +on o.order_id = oi.order_item_order_id \
 +group by o.order_date order by o.order_date")
 +
 +for data in joinAggData.collect():
    print(data)
  
  ##############################################################################
 @@ -171,6 +236,12 @@ ordersAggRDD = joinData.reduceByKey( \
  lambda total1, total2: total1 + total2 \
  )
  
 +ordersAggRDD = joinData.aggregateByKey( \
 +(0, 0), \
 +lambda subtotal1, subtotal2: (subtotal1[0] + subtotal2, subtotal1[1] + 1), \
 +lambda total1, total2: (round(total1[0] + total2[0], 2), total1[1] + total2[1]) \
 +)
 +
  for data in ordersAggRDD.collect():
    print(data)
  
 @@ -184,6 +255,12 @@ ordersAvgJoinData = ordersAggRDD.join(ordersPerDayRDD)
  for data in ordersAvgJoinData.map(t => (t[0], t[1][0]/t[1][1])).sortByKey().collect():
    print data
  
 +
 +# Using Hive Context
 +from pyspark.sql import HiveContext
 +sqlContext = HiveContext(sc)
 +
 +query = "select 
  ##########################################################################################
  
  # Using data frames (only works with spark 1.3.0 or later)
View
17  hadoop/edw/cloudera/sqoop/sqoop_demo.txt
 @@ -63,6 +63,23 @@ TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/sqoop
  
  -- It will create tables in default database in hive
  -- Using snappy compression
 +-- As we have imported all tables before make sure you drop the directories
 +-- Launch hive drop all tables
 +drop table departments;
 +drop table categories;
 +drop table products;
 +drop table orders;
 +drop table order_items;
 +drop table customers;
 +
 +-- Dropping directories, in case your hive database/tables in consistent state
 +hadoop fs -rm -R /user/hive/warehouse/departments
 +hadoop fs -rm -R /user/hive/warehouse/categories
 +hadoop fs -rm -R /user/hive/warehouse/products
 +hadoop fs -rm -R /user/hive/warehouse/orders 
 +hadoop fs -rm -R /user/hive/warehouse/order_itmes
 +hadoop fs -rm -R /user/hive/warehouse/customers
 +
  sqoop import-all-tables \
    --num-mappers 1 \
    --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
