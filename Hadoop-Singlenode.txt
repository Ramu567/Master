Hadoop Instalation:
===================
Following are the list of steps that will take to get this done:
1) Install Java
2) Add a dedicated Hadoop user
3) Install SSH and setup SSH certificates
4) Check if SSH works
5) Install Hadoop
6) Modify Hadoop config files
7) Format Hadoop filesystem
8) Start Hadoop
9) Check Hadoop through web UI
10) Stop Hadoop

Step 1 – Install Java and Set JAVA_HOME
-----------------------------------------
Let’s go ahead and install Java on our ubuntu machine.
ubuntu@ip-172-31-42-156:~$ sudo add-apt-repository ppa:webupd8team/java
ubuntu@ip-172-31-42-156:~$ sudo apt-get update
ubuntu@ip-172-31-42-156:~$ sudo apt-get install oracle-java8-installer
ubuntu@ip-172-31-42-156:~$ java -version
java version "1.8.0_95"
OpenJDK Runtime Environment (IcedTea 2.6.4) (7u95-2.6.4-0ubuntu0.15.10.1)
OpenJDK 64-Bit Server VM (build 24.95-b01, mixed mode)

Step 2 – Add a dedicated Hadoop user
------------------------------------
ubuntu@ip-172-31-42-156:~$  sudo addgroup hadoop
Adding group `hadoop' (GID 1002) ...
Done.
 
ubuntu@ip-172-31-42-156:~$ sudo adduser --ingroup hadoop hadoop
Adding user `hadoop' ...
Adding new user `hadoop' (1001) with group `hadoop' ...
Creating home directory `/home/hadoop' ...
Copying files from `/etc/skel' ...
Enter new UNIX password: 
Retype new UNIX password: 
passwd: password updated successfully
Changing the user information for hadoop
Enter the new value, or press ENTER for the default
	Full Name []: 
	Room Number []: 
	Work Phone []: 
	Home Phone []: 
	Other []: 
Is the information correct? [Y/n] Y

Add hduser to sudo user group:
------------------------------
ubuntu@ip-172-31-42-156:/home/ubuntu$ sudo adduser hadoop sudo
[sudo] password for hadoop: 
Adding user `hadoop' to group `sudo' ...
Adding user hadoop to group sudo
Done.
Step 3 – Install SSH and Create Certificates:
---------------------------------------------
ubuntu@ip-172-31-42-156:~$ sudo apt-get install ssh

ubuntu@ip-172-31-42-156:~$ su hadoop
Password:hadoop 

to change password for hadoop:'
------------------------------
ubuntu@ip-172-31-42-156:~$ sudo passwd hadoop

ubuntu@ip-172-31-42-156:~$ ssh-keygen -t rsa -P ""

Generating public/private rsa key pair.
Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): 
Created directory '/home/hduser/.ssh'.
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
The key fingerprint is:
50:6c:e3:fd:0f:65:bf:30:81:c2:41:71:26:we:7d:q6 hadoop@ip-172-31-42-156
The key's randomart image is:
+--[ RSA 2048]----+
|        .oo.o    |
|       . .o=. o  |
|      . + .  o . |
|       o =    E  |
|        S +      |
|         . +     |
|          O +    |
|           O o   |
|            o..  |
+-----------------+
 
hadoop@ip-172-31-42-156:/home/hadoop$ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

Step 4 – Check if SSH works:
-----------------------------
hadoop@ip-172-31-42-156:/home/hadoop$ ssh localhost

The authenticity of host 'localhost (127.0.0.1)' can't be established.
ECDSA key fingerprint is 50:6c:e3:fd:0f:65:bf:30:81:c2:41:71:26:we:7d:q6.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.
Welcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-40-generic x86_64)

Step 5 – Install Hadoop:
------------------------
hadoop@ip-172-31-42-156:/home/hadoop$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz
hadoop@ip-172-31-42-156:/home/hadoop$ tar xvzf hadoop-2.6.0.tar.gz
hadoop@ip-172-31-42-156:/home/hadoop$ sudo mv * /usr/local/hadoop
ubuntu@ip-172-31-42-156:/home/ubuntu$ mkdir /usr/local/hadoop
hadoop@ip-172-31-42-156:/home/hadoop$ sudo chown -R hadoop:hadoop /usr/local/hadoop

Step 6 – Modify Hadoop Config Files:
------------------------------------
We are going to modify following files:
1) ~/.bashrc
2) /usr/local/hadoop/hadoop/etc/hadoop/hadoop-env.sh
3) /usr/local/hadoop/hadoop/etc/hadoop/hdfs-site.xml
4) /usr/local/hadoop/hadoop/etc/hadoop/core-site.xml
5) /usr/local/hadoop/hadoop/etc/hadoop/mapred-site.xml.template
6) /usr/local/hadoop/hadoop/etc/hadoop/yarn-site.xml
1) .bashrc
=======
Append following lines to the end of  ~/.bashrc:

hadoop@ip-172-31-42-156:/home/hadoop$ sudo vi ~/.bashrc

#HADOOP VARIABLES START

export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_INSTALL=/usr/local/hadoop/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"

#HADOOP VARIABLES END
----------------------------------
#Reload .bashrc
hadoop@ip-172-31-42-156:/home/hadoop$ source ~/.bashrc

2) hadoop-env.sh
-----------------
Set JAVA_HOME in hadoop-env.sh file.

hadoop@ip-172-31-42-156:/home/hadoop$ vi /usr/local/hadoop/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-oracle

3) hdfs-site.xml
-----------------
The hdfs-site.xml file needs to be configured for each host in the cluster. 
Here you specify the directories which will be used as the namenode and the datanode on the host.

Before we edit hdfs-site.xml, we need to create two directories which will contain the namenode and the datanode.

ubuntu@ip-172-31-42-156:/home/ubuntu$ sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode
ubuntu@ip-172-31-42-156:/home/ubuntu$ sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode
ubuntu@ip-172-31-42-156:/home/ubuntu$ sudo mkdir -p /usr/local/hadoop_store/hdfs/secondaryNamenode
ubuntu@ip-172-31-42-156:/home/ubuntu$ sudo chown -R hadoop:hadoop /usr/local/hadoop_store

Now open hdfs-site.xml and paste following between configuration tags:
----------------------------------------------------------------------
hadoop@ip-172-31-42-156:/home/hadoop$ vi /usr/local/hadoop/hadoop/etc/hadoop/hdfs-site.xml

 <configuration>
 <property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
 </property>
 <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
 </property>
 <property>   
 <name>fs.checkpoint.dir</name>  
 <value>file:/user/local/hadoop_store/hdfs/secondaryNamenode</value>
 </property>
 <property>   
 <name>fs.checkpoint.edits.dir</name>  
 <value>file:/user/local/hadoop_store/hdfs/secondaryNamenode</value> 
 </property> 
</configuration>

4) core-site.xml
---------------------
Now create the tmp directory and set the required permissions:

ubuntu@ip-172-31-42-156:/home/ubuntu$ sudo mkdir -p /app/hadoop/tmp
ubuntu@ip-172-31-42-156:/home/ubuntu$ sudo chown hadoop:hadoop /app/hadoop/tmp

Paste the following in between the <configuration></configuration> tag of core-site.xml file:

hadoop@ip-172-31-42-156:/home/hadoop$ vi /usr/local/hadoop/hadoop/etc/hadoop/core-site.xml

<configuration>
 <property>
  <name>hadoop.tmp.dir</name>
  <value>/app/hadoop/tmp</value>
  <description>A base for other temporary directories.</description>
 </property>
 <property>
  <name>fs.default.name</name>
  <value>hdfs://localhost:54310</value>
  <description>The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.</description>
 </property>
</configuration>

5) mapred-site.xml.template
----------------------------

The /usr/local/hadoop/hadoop/etc/hadoop/ folder contains /usr/local/hadoop/hadoop/etc/hadoop/mapred-site.xml.template file 
which has to be renamed or copied with the name mapred-site.xml:

hadoop@ip-172-31-42-156:/home/hadoop$ mv /usr/local/hadoop/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/hadoop/etc/hadoop/mapred-site.xml
Add following between configuration tags in mapred-site.xml file:

hadoop@ip-172-31-42-156:/home/hadoop$ vi /usr/local/hadoop/hadoop/etc/hadoop/mapred-site.xml

<configuration>
 <property> 
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
 </property>
</configuration>
6) yarn-site.xml
-------------------

This file is used to configure yarn into Hadoop. Open the yarn-site.xml file and 
add the following properties in between the <configuration>, </configuration> tags in this file.
hadoop@ip-172-31-42-156:/home/hadoop$ vi /usr/local/hadoop/hadoop/etc/hadoop/yarn-site.xml

<configuration>
   <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value> 
   </property>  
</configuration>

Step 7 – Format Hadoop Filesystem:
--------------------------------------
Before we start Hadoop we need to format the Hadoop filesystem. The format command creates current directory under /usr/local/hadoop_store/hdfs/namenode.

hadoop@ip-172-31-42-156:/home/hadoop$ cd ~ 

hadoop@ip-172-31-42-156:~$ hdfs namenode -format 

Step 8 – Start Hadoop:
---------------------
hadoop@ip-172-31-42-156:~$ cd /usr/local/hadoop/sbin
hadoop@ip-172-31-42-156:/usr/local/hadoop/sbin$ start-all.sh
                  (or)
hadoop@ip-172-31-42-156:/usr/local/hadoop/sbin$ start-dfs.sh
hadoop@ip-172-31-42-156:/usr/local/hadoop/sbin$ start-yarn.sh

Step 9 – Stop Hadoop:
---------------------
hadoop@ip-172-31-42-156:/usr/local/hadoop/sbin$ stop-all.sh
            (or)
hadoop@ip-172-31-42-156:/usr/local/hadoop/sbin$ stop-dfs.sh
hadoop@ip-172-31-42-156:/usr/local/hadoop/sbin$ stop-yarn.sh
----------------------------------------------------------
You might be asked to accept machine’s key.

Check if everything is running::
--------------------------------
hadoop@ip-172-31-42-156:/usr/local/hadoop/sbin$ jps

You should get something like:
-----------------------------
7809 Jps
6850 NameNode
7224 SecondaryNameNode
7018 DataNode
7531 NodeManager
7375 ResourceManager

For WebHDFS configrations:
-------------------------
hdfs-site.xml
--------------
<property>
<name>dfs.webhdfs.enabled</name>
<value>true</value>
</property>

<property>
<name>dfs.support.append</name>
<value>true</value>
</property>

<property>
<name>dfs.permissions</name>
<value>false</value>
</property>

Authentication for Hadoop HTTP web-consoles:
----------------------------------------
core-site.xml
-------------
<property>
<name>hadoop.http.authentication.type</name>
<value>simple</value>
</property>

<property>
<name>hadoop.http.authentication.token.validity</name>
<value>36000</value>
</property>

<property>
<name>hadoop.http.authentication.simple.anonymous.allowed</name>
<value>true</value>
</property>

CORS:
--------
To enable cross-origin support (CORS), please set the following configuration parameters:
Add org.apache.hadoop.security.HttpCrossOriginFilterInitializer to hadoop.http.filter.initializers in core-site.xml. 
You will also need to set the following properties in core-site.xml 


<property>
<name>hadoop.http.cross-origin.enabled</name>
<value>true</value>
</property>

<property>
<name>hadoop.http.cross-origin.allowed-origins</name>
<value>*</value>
</property>

<property>
<name>hadoop.http.cross-origin.allowed-methods</name>
<value>GET,POST,HEAD</value>
</property>
<property>
<name>hadoop.http.cross-origin.allowed-headers</name>
<value>X-Requested-With,Content-Type,Accept,Origin</value>
</property>

<property>
<name>hadoop.http.cross-origin.max-age</name>
<value>1800</value>
</property>









